{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\n\n#nlp\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\neng_stopwords = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    test = pd.read_csv(test_path)\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('../input/jigsaw-toxic-comment-classification-challenge/train.csv', \n                                            '../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Any missing values in independent variables?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to add \"normal\" variable to identify comments which aren't toxic."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['normal'] = np.where((data.toxic==0) & (data.threat==0) & (data.severe_toxic==0) & \n                          (data.obscene==0) & (data.insult==0) & (data.identity_hate==0),1,0)\ndata['normal'] = np.where((data.toxic.isnull()),np.nan,data['normal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we need to unify the text, getting rid of capital letters and shortened forms. For this purpose the function \"cleaning_comments\" will be defined."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaning_comments(data, text):\n    import re\n    data[text] = data[text].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    data[text] = data[text].apply(lambda x: re.sub(r\"what's\", \"what is \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'s\", \" \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'ve\", \" have \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"can't\", \"cannot \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'re\", \" are \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"n't\", \" not \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"i'm\", \"i am \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'d\", \" would \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'ll\", \" will \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"you're\", \"you are \", x))\n    \ncleaning_comments(data, 'comment_text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look how each kind of toxic comment like"},{"metadata":{"trusted":true},"cell_type":"code","source":"def example_of_comment(data, kind_of_comment, name_of_category):\n    print (name_of_category, data[data[kind_of_comment]==1].iloc[0,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'toxic', 'Toxic comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'threat', 'Threat comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'severe_toxic', 'Severe toxic comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'obscene', 'Obscene comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'insult', 'Insult comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'identity_hate', 'Identity hate comment:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_of_comment(data,'normal', 'Normal comment:')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this comment \"89.205.38.27\" looks like IP address, so it's something to check later."},{"metadata":{},"cell_type":"markdown","source":"Correlation between category of comment"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_of_comment = data.drop(['comment_text','id'],axis=1)\n\ncorrelation=columns_of_comment.corr()\nplt.figure(figsize=(8,6))\nsns.heatmap(correlation,\n            xticklabels=correlation.columns.values,\n            yticklabels=correlation.columns.values, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how it looks in number"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crosstab(data, columns_of_comment, main_variable):\n    corr=[]\n    for other_variables in columns_of_comment.columns:\n        confusion_matrix = pd.crosstab(columns_of_comment[main_variable], columns_of_comment[other_variables])\n        corr.append(confusion_matrix)\n    output = pd.concat(corr,axis=1,keys=columns_of_comment.columns)\n    return output\n\ncrosstab(data, columns_of_comment, \"toxic\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cane see that every toxic comment is also severe toxic. "},{"metadata":{},"cell_type":"markdown","source":"To show the most popular words in each category of toxic comments I'm gonna use word clouds."},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, plot_title):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=60, \n        scale=3,\n        random_state=1).generate(str(data))\n    fig = plt.figure(1, figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(wordcloud)\n    plt.title(plot_title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(data['comment_text'][data.toxic==1],\"Toxic comments\")\nshow_wordcloud(data['comment_text'][data.threat==1],\"Threat comments\")\nshow_wordcloud(data['comment_text'][data.insult==1],\"Insult comments\")\nshow_wordcloud(data['comment_text'][data.obscene==1],\"Obscene comments\")\nshow_wordcloud(data['comment_text'][data.severe_toxic==1],\"Severe toxic comments\")\nshow_wordcloud(data['comment_text'][data.identity_hate==1],\"Identity hate comments\")\nshow_wordcloud(data['comment_text'][data.normal==1],\"Normal comments\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I want to check how it looks after dropping common words, but I don't want to drop it from data now, so let's make new dataset - data_working, drop common words and make the same word clouds."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_working = data.copy()\nfrequence = pd.Series(' '.join(data_working['comment_text']).split()).value_counts()[:300]\nfrequence = list(frequence.index)\ndata_working['comment_text'] = data_working['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in frequence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(data_working['comment_text'][data_working.toxic==1],\"Toxic comments\")\nshow_wordcloud(data_working['comment_text'][data_working.threat==1],\"Threat comments\")\nshow_wordcloud(data_working['comment_text'][data_working.insult==1],\"Insult comments\")\nshow_wordcloud(data_working['comment_text'][data_working.obscene==1],\"Obscene comments\")\nshow_wordcloud(data_working['comment_text'][data_working.severe_toxic==1],\"Severe toxic comments\")\nshow_wordcloud(data_working['comment_text'][data_working.identity_hate==1],\"Identity hate comments\")\nshow_wordcloud(data_working['comment_text'][data_working.normal==1],\"Normal comments\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a few new variables (with a few hypotheses):\n\n* Number of words - haters usually write longer messages\n* Number of characters (with spaces) - haters usually write longer messages\n* Stop words and number of them\n* Uppercase words - haters often use caps lock\n* Number of unique words - haters often spam\n* Frequency of unique words (comments with freq value below 30% can be something lik spam)\n* Number of title\n* Number of punctuations - haters use it rarely\n* Average length of words\n* Number of smiles - haters use it rarely\n* Number of exclamation marks - popular among haters\n* Number of question marks - haters use it rarely"},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_variables(data, variable):\n    \n    data['comment_text_words'] = data[variable].apply(lambda x: len(str(x).split(\" \")))\n    \n    data['comment_text_chars'] = data[variable].str.len()\n    \n    data['comment_text_unique_words'] = data[variable].apply(lambda x: len(set(str(x).split())))\n    \n    data['comment_text_freq_of_unique_words'] = data['comment_text_unique_words'] / data['comment_text_words'] * 100\n    \n    data['spammer'] = np.where(data['comment_text_freq_of_unique_words']<20,1,0)\n    \n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n    data['comment_text_stopwords'] = data[variable].apply(lambda x: len([x for x in x.split() if x in stop]))\n    \n    data['comment_text_uppers'] = data[variable].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    \n    data['comment_text_title'] = data[variable].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    \n    data['comment_text_punctuation'] = data[variable].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    \n    data['comment_text_avg_length'] = data[variable].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    \n    data['comment_text_smile'] = data[variable].apply(lambda x: sum(x.count(w) for w in (':-)', ':)', ';-)', ';)')))\n    \n    data['comment_text_exclamation'] = data[variable].apply(lambda x: x.count('!'))\n    \n    data['comment_text_question'] = data[variable].apply(lambda x: x.count('?'))\n    \n    \nnew_variables(data, 'comment_text')\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = ('comment_text_words','comment_text_chars','comment_text_unique_words',\n             'comment_text_freq_of_unique_words','spammer','comment_text_stopwords',\n             'comment_text_uppers', 'comment_text_title','comment_text_punctuation',\n             'comment_text_avg_length', 'comment_text_smile','comment_text_exclamation','comment_text_question')\ntypes_of_comment = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','normal')\n\nrows = [{c:data[f].corr(data[c]) for c in types_of_comment} for f in variables]\ndata_correlations = pd.DataFrame(rows, index=variables)\ndata_correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def number_of_comments_by_spammer(data, types, spam_variable):\n    comments_by_spammer = pd.DataFrame({'Kind of comment': [],\n                                        'Number of comments by spammer': []})\n\n    for type_of_comment in types:\n        num_of_comm_by_spammer = data[type_of_comment][(data[spam_variable]==1) & (data[type_of_comment]==1)].count() \n        comments_by_spammer=comments_by_spammer.append({'Kind of comment': type_of_comment, \n                                                        'Number of comments by spammer': num_of_comm_by_spammer},\n                                                        ignore_index=True)\n    return comments_by_spammer\n\nnumber_of_comments_by_spammer(data, ['identity_hate','insult','obscene','severe_toxic','threat','toxic','normal'], 'spammer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Toxic comments are mainly written by spammers."},{"metadata":{},"cell_type":"markdown","source":"*Is there a relationship between comment length and toxicity?*\n\nI will prepare a table that will contain the types of comments, the average number of words in the comments and the average number of characters in the comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_number_of_words_and_chars_in_each_type_of_comment(data, types):\n    avg_number_of_words_in_each_type_of_comment = pd.DataFrame({'Kind of comment': [],\n                                                                'Average number of words in comment': [],\n                                                                'Average number of chars in comment': []})\n    for type_of_comment in types:\n        number_of_words = data.comment_text_words[data[type_of_comment]==1].sum()\n        number_of_comments = data.comment_text[data[type_of_comment]==1].count()\n        avg_words_number = number_of_words/number_of_comments\n        \n        number_of_chars = data.comment_text_chars[data[type_of_comment]==1].sum()\n        avg_chars_number = number_of_chars/number_of_comments\n        \n        avg_number_of_words_in_each_type_of_comment = avg_number_of_words_in_each_type_of_comment.append({'Kind of comment': type_of_comment, \n                                                        'Average number of words in comment': avg_words_number,\n                                                        'Average number of chars in comment': avg_chars_number},ignore_index=True)\n    return avg_number_of_words_in_each_type_of_comment\n\naverage_number_of_words_and_chars_in_each_type_of_comment(data, ['identity_hate','insult','obscene','severe_toxic','threat','toxic','normal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is difficult to notice any clear correlation here."},{"metadata":{},"cell_type":"markdown","source":"The comments contain IP addresses, links and usernames that are unique and do not contribute to analysis. We should extract these things and remove them from comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['username']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fixing_values_in_variable(data, variable):\n    data[variable] = data[variable].map(lambda x: None if len(x)==0 else x)\n    data[variable] = data[variable].map(lambda x: x[0] if x!=None else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fixing_values_in_variable(data, 'username')\ndata['link']=data[\"comment_text\"].apply(lambda x: re.findall(\"http://.*com\",str(x)))\nfixing_values_in_variable(data, 'link')\ndata['ip']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\nfixing_values_in_variable(data, 'ip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing IP, links and usernames"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"\\[\\[(.*)\\|\",\"\",str(x)))\ndata['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"http://.*com\",\"\",str(x)))\ndata['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",str(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now is time to basic pre-processing, because present comment text is something like a mess.\n\n* Removing punctuation\n* Removing stop words\n* Removing common words\n* Removing white spaces\n* Removing numbers\n* Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_processing(data,variable,count):\n\n    data[variable] = data[variable].str.replace('[^\\w\\s]','')\n\n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n    data[variable] = data[variable].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n    frequence = pd.Series(' '.join(data[variable]).split()).value_counts()[:count]\n    frequence = list(frequence.index)\n    data[variable] = data[variable].apply(lambda x: \" \".join(x for x in x.split() if x not in frequence))\n\n    data[variable] = data[variable].map(lambda x: x.strip())\n\n    import re\n    data[variable] = data[variable].map(lambda x: re.sub(r'\\d+', '', x))\n\n    from textblob import Word\n    data[variable] = data[variable].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    \n    \npre_processing(data,'comment_text',100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading.\n\nStay tuned :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}